{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of M1E3_Stochastic_Minibatch_GD.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganilkumar09/aiml/blob/master/Anil_M1E3_Stochastic_Minibatch_GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UERx5ALuTJW_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "metadata": {
        "id": "2U7msw6GTJXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The objective of this experiments is we will be using Stochastic Gradient Descent Method in which every next point is chosen randomly and we will be using mini batch algorithm. In every iteration we use a set of 'm' training examples called batch to compute the gradient of the cost function. "
      ]
    },
    {
      "metadata": {
        "id": "mDRTMSzJTJXD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this experiment we will work with simple pendulum dataset. We note that this is actual experimental data for the **Simple Pendulum** experiment; the length in cm and the period of oscillation in seconds are the two columns of data. So we will use the names **l** and **t** from now on. Recall that $L \\propto T^2$"
      ]
    },
    {
      "metadata": {
        "id": "mhOdzhI9TJXF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Keywords\n",
        "\n",
        "* Stochastic Gradient Descent\n",
        "* Scipy\n",
        "* Sklearn\n",
        "* mini batch Gradient Descent\n",
        "* Plotting Error vs Iteration"
      ]
    },
    {
      "metadata": {
        "id": "s0Mvlz1fTJXF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Expected Time : 90 mins"
      ]
    },
    {
      "metadata": {
        "id": "fCtf1KWE40lx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup Steps"
      ]
    },
    {
      "metadata": {
        "id": "98uM_xF8426S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Km69XLfr5BGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Xp91vRMpNo1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook=\"M1E3_Stochastic_Minibatch_GD\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://www.dropbox.com/s/wqxu1h85e9w39ab/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt.zip?dl=1\")\n",
        "    ipython.magic(\"sx unzip AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt.zip?dl=1\")\n",
        "    print (\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    \n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      print(\"Your submission is successful.\")\n",
        "      print(\"Ref Id:\", submission_id)\n",
        "      print(\"Date of submission: \", r[\"date\"])\n",
        "      print(\"Time of submission: \", r[\"time\"])\n",
        "      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n",
        "      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if Additional: return Additional      \n",
        "    else: raise NameError('')\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "  \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sRjAjk1hTJXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing required Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stat\n",
        "%matplotlib notebook\n",
        "from  matplotlib import pyplot as plt\n",
        "import random\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLdAeF_tTJXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read the data"
      ]
    },
    {
      "metadata": {
        "id": "YDNrlfakTJXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n",
        "print(data.head())\n",
        "print(data.tail())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTtgOro7TJXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = data['l'].values\n",
        "t = data['t'].values\n",
        "tsq = t * t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srgyaBxoTJXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stochastic gradient descent (Single sample)**\n",
        "\n",
        "Instead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random."
      ]
    },
    {
      "metadata": {
        "id": "hpHa2ZwyTJXX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " $y_i = mx_i + c$\n",
        "\n",
        "$E$ = $(y - y_i)^2$\n",
        "\n",
        "$\\frac{\\partial E }{\\partial m}$ = $ -(y - (mx_i + c)) * x_i$\n",
        "\n",
        "$\\frac{\\partial E }{\\partial c}$ = $ -(y - (mx_i + c))$"
      ]
    },
    {
      "metadata": {
        "id": "hUq_mCNLTJXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_step(x, y, m, c, eta):\n",
        "    ycalc = m * x + c\n",
        "    error = (y - ycalc) ** 2\n",
        "    delta_m = -(y - ycalc) * x\n",
        "    delta_c = -(y - ycalc)\n",
        "    m = m - delta_m * eta\n",
        "    c = c - delta_c * eta\n",
        "    return m, c, error\n",
        "\n",
        "def one_loop_random(x, y, m, c, eta):\n",
        "    # Making random idx\n",
        "    random_idx = np.arange(len(y))\n",
        "    np.random.shuffle(random_idx)\n",
        "    # Training with random idx\n",
        "    for idx in random_idx:\n",
        "        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n",
        "        #print(m, c, e)\n",
        "    return m,c,e\n",
        "\n",
        "def train_stochastic(x, y, m, c, eta, iterations=1000):\n",
        "    for iteration in range(iterations):\n",
        "        m, c, err = one_loop_random(x, y, m, c, eta)\n",
        "    return m, c, err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HM2O8HtFTJXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TRAIN"
      ]
    },
    {
      "metadata": {
        "id": "JlR-YQJoTJXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Init m, c\n",
        "m, c = 0, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kijRetDpTJXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "lr = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8FDMh5YtTJXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training for 1000 iterations, plotting after every 100 iterations:\n",
        "%matplotlib inline\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ion()\n",
        "fig.show()\n",
        "fig.canvas.draw()\n",
        "\n",
        "for num in range(10):\n",
        "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100)\n",
        "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
        "    y = m * l + c\n",
        "    ax.clear()\n",
        "    ax.plot(l, tsq, '.k')\n",
        "    ax.plot(l, y)\n",
        "    fig.canvas.draw()\n",
        "    time.sleep(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCVJrwLQTJXo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 1: Experiment with other lr values.**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7vU_e7ilTJXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GRP1Ge9TJXr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 2: plot Errors vs Iterations**"
      ]
    },
    {
      "metadata": {
        "id": "SXceKp3zTJXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your Code Here\n",
        "ms, cs,errs = [], [], []\n",
        "m, c = 0, 0\n",
        "lr = 0.001\n",
        "for times in range(100):\n",
        "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) # We will plot the value of for every 100 iterations\n",
        "    ms.append(m)\n",
        "    cs.append(c)\n",
        "    errs.append(error)\n",
        "epochs = range(0, 10000, 100)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, errs)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Sequential Gradient Descent\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9rIMmaMTJXv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 3 : Is this better than sequential gradient descent and vanilla gradient descent?**\n",
        "\n",
        "Hint - check the error value at saturation, and time it takes to reach saturation."
      ]
    },
    {
      "metadata": {
        "id": "_jhCKU75TJXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Last Error at saturation: 0.004"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwIZ7tuZTJXx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PROBLEM\n",
        "\n",
        "Problem with Sequential/Stochastic Gradient Descent is it does not scale well - it makes the same calculation of gradient descent on each sample. So the time taken will increase linearly with the number of samples. Many datasets have samples in the range of millions. Hence, even though it gives good results, it is not ideal.\n",
        "\n",
        "We need a gradient descent formulation that gives the speed of vanilla gradient descent and the accuracy of sequential/stochastic gradient descent.\n",
        "\n",
        "Next we will see **Minibatch Gradient Descent!**"
      ]
    },
    {
      "metadata": {
        "id": "m3S-IvdHTJXx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Minibatch Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "ZT6lGuO8TJXy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration we use a subset of training examples (called \"batch\") to compute the gradient of the cost function. \n",
        "\n",
        "Common mini-batch sizes range between 50 and 256, but can vary for different applications."
      ]
    },
    {
      "metadata": {
        "id": "8IWur68qTJXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "one_batch() : we will be calculating the essenial parts of the Gradient Descent method:  \n",
        "\n",
        "$y_i = mx_i + c$\n",
        "        \n",
        "$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y - y_i)^2$\n",
        "\n",
        "$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y - (mx_i + c))$\n",
        " \n",
        "$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y - (mx_i + c))$"
      ]
    },
    {
      "metadata": {
        "id": "mJgPCef6TJXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "one_step() : We will be splitting our data into batches."
      ]
    },
    {
      "metadata": {
        "id": "JkRMHxA2TJX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_one_batch(x, y, m, c, eta):\n",
        "    const = - 2.0/len(y)\n",
        "    ycalc = m * x + c\n",
        "    delta_m = const * sum(x * (y - ycalc))\n",
        "    delta_c = const * sum(y - ycalc)\n",
        "    m = m - delta_m * eta\n",
        "    c = c - delta_c * eta\n",
        "    error = sum((y - ycalc)**2)/len(y)\n",
        "    return m, c, error\n",
        "\n",
        "def train_batches(x, y, m, c, eta, batch_size):\n",
        "    # Making the batches\n",
        "    random_idx = np.arange(len(y))\n",
        "    np.random.shuffle(random_idx)\n",
        "    \n",
        "    # Train each batch\n",
        "    for batch in range(len(y)//batch_size):\n",
        "        batch_idx = random_idx[batch*batch_size:(batch+1)*batch_size]\n",
        "        batch_x = x[batch_idx]\n",
        "        batch_y = y[batch_idx]\n",
        "        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)\n",
        "    \n",
        "    return m, c, err\n",
        "\n",
        "def train_minibatch(x, y, m, c, eta, batch_size=10, iterations=1000):\n",
        "    for iteration in range(iterations):\n",
        "        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n",
        "    return m, c, err\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BLipYesTJX4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### TRAIN"
      ]
    },
    {
      "metadata": {
        "id": "oAsrGHdLTJX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Init m, c\n",
        "m, c = 0, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXm2ZSEwTJX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "lr = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0ejZ17GTJX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "batch_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Y-XhUFqTJYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# Training for 1000 iterations, plotting after every 100 iterations:\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "plt.ion()\n",
        "fig.show()\n",
        "fig.canvas.draw()\n",
        "\n",
        "for num in range(10):\n",
        "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=90, iterations=100)\n",
        "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
        "    y = m * l + c\n",
        "    ax.clear()\n",
        "    ax.plot(l, tsq, '.k')\n",
        "    ax.plot(l, y)\n",
        "    fig.canvas.draw()\n",
        "    time.sleep(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPrP3o7JTJYF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 4: Experiment with other lr values.**"
      ]
    },
    {
      "metadata": {
        "id": "zmQ_eNyQTJYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "stwbu3wYTJYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 5: Experiment with other batch_size values.**"
      ]
    },
    {
      "metadata": {
        "id": "uBUcyH_QTJYJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vIFFYGKnTJYL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Plotting error vs iterations"
      ]
    },
    {
      "metadata": {
        "id": "OhcEs6pVTJYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "ms, cs,errs = [], [], []\n",
        "m, c = 0, 0\n",
        "lr = 0.001\n",
        "batch_size = 10\n",
        "for times in range(100):\n",
        "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size, iterations=100) # We will plot the value of Error for every 100 iterations\n",
        "    ms.append(m)\n",
        "    cs.append(c)\n",
        "    errs.append(error)\n",
        "epoch = range(0, 10000, 100)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epoch, errs)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Minibatch Gradient Descent\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYuMICffTJYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Ungraded Exercise 6: Is this better than sequential gradient descent and vanilla gradient descent?**\n",
        "\n",
        "Hint - check the error value at saturation, and time it takes to reach saturation."
      ]
    },
    {
      "metadata": {
        "id": "sYqSG6SJTJYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### Last Error at saturation: 0.006"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VKXYIXL5zCi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "metadata": {
        "id": "UYhUFOC2qDdb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title For very large datasets, which of the following gradient descent methods is recommended? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"Stochastic\",\"Mini-batch\",\"Batch\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9yfq4Cc5zqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E9zKHoZa52UE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Gb5zgRl54z-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"Yes\", \"No\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oiw7_YW958PG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id =return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdwVqNNGDP6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}